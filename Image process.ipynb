{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa4a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "DATADIR = r\"C:\\Users\\acer\\Downloads\\w1867202\\SDGP\\Dataset\"\n",
    "CATEGORIES = ['Chickenpox','Mild','Monkeypox','Normal','Severe']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c2a2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)  # Specify the desired image size\n",
    "batch_size = 32  \n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d31a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_train = CATEGORIES\n",
    "\n",
    "# Create a mapping from class labels to integers\n",
    "class_labels = np.unique(y_train)\n",
    "label_to_integer = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "# Convert the string labels to integers\n",
    "y_train_int = [label_to_integer[label] for label in y_train]\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(class_labels), y= class_labels)\n",
    "class_weights_dict = dict(zip(np.unique(y_train_int), class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d5cf8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define custom loss function with class weights\n",
    "def weighted_categorical_crossentropy(class_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Convert class weights to float32\n",
    "        class_weights_float32 = tf.cast(class_weights, dtype=tf.float32)\n",
    "\n",
    "        # Calculate the cross-entropy loss for each sample\n",
    "        ind_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply class weights to the loss for each sample\n",
    "        weights = tf.gather(class_weights_float32, tf.argmax(y_true, axis=1))\n",
    "        weighted_loss = ind_loss * weights\n",
    "\n",
    "        # Return the average loss over all samples\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "225a3c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1328 images belonging to 5 classes.\n",
      "Found 32 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "# traindata = training_data\n",
    "train_data = trdata.flow_from_directory(directory=\"Dataset\",target_size=(224,224))\n",
    "\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data = tsdata.flow_from_directory(directory=\"test\", target_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2151c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1328 images belonging to 5 classes.\n",
      "Found 0 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "#Data Augmentation Generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the Training and Validation Data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    directory=\"Dataset\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    "    shuffle=True,  \n",
    "    subset='training'  \n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    directory=\"Dataset\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    "    shuffle=True,  \n",
    "    subset='validation'  \n",
    ")\n",
    "\n",
    "# Separate X_val and y_val from val_data generator\n",
    "X_val, y_val = [], []\n",
    "for _ in range(len(val_data)):\n",
    "    x_val_batch, y_val_batch = next(val_data)\n",
    "    X_val.extend(x_val_batch)\n",
    "    y_val.extend(y_val_batch)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85e5eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16 model \n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8f8855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "IMAGE_SIZE=[224,224]\n",
    "num_classes = 3\n",
    "vgg=VGG16(input_shape=IMAGE_SIZE+[num_classes],weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2737840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78b5797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8af6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "folders=glob(r\"C:\\Users\\acer\\Downloads\\w1867202\\SDGP\\Dataset\\*\")\n",
    "print(len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a330a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 125445    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,840,133\n",
      "Trainable params: 125,445\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add custom classification layers\n",
    "\n",
    "x=Flatten()(vgg.output)\n",
    "prediction=Dense(len(folders),activation='softmax')(x)\n",
    "model=Model(inputs=vgg.input,outputs=prediction)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0d61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4c75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ADAM optimizer & compile \n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=weighted_categorical_crossentropy(class_weights),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e0268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c67cdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff3de55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 435s 10s/step - loss: 1.0776 - accuracy: 0.6212\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 442s 11s/step - loss: 0.6067 - accuracy: 0.7658\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 431s 10s/step - loss: 0.4886 - accuracy: 0.8283\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 431s 10s/step - loss: 0.4655 - accuracy: 0.8261\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 433s 10s/step - loss: 0.4116 - accuracy: 0.8502\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 430s 10s/step - loss: 0.4136 - accuracy: 0.8517\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 431s 10s/step - loss: 0.2903 - accuracy: 0.9074\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 430s 10s/step - loss: 0.3402 - accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 433s 10s/step - loss: 0.2654 - accuracy: 0.9142\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 430s 10s/step - loss: 0.2605 - accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "# Train ML model\n",
    "\n",
    "trained_model = model.fit(\n",
    "    x=train_data,                   \n",
    "    epochs=10,              \n",
    "    validation_data=val_data,        \n",
    "    steps_per_epoch=len(train_data), \n",
    "    validation_steps=len(val_data)   \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c0ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b38df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264a2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb332fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6a282cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 495ms/step\n",
      "Monkeypox\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "CATEGORIES = ['Chickenpox', 'Mild', 'Monkeypox', 'Normal', 'Severe']\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = image.load_img(r\"C:\\Users\\acer\\Downloads\\w1867202\\SDGP\\test\\Monkeypox\\Monkeypox3.png\", target_size=(IMG_SIZE, IMG_SIZE))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img.astype('float32') / 255.0\n",
    "\n",
    "# Load the pre-trained model\n",
    "with tf.keras.utils.custom_object_scope({\"loss\": weighted_categorical_crossentropy}):\n",
    "    model = load_model('saved_model.h5')\n",
    "\n",
    "# Get the prediction\n",
    "predicted_img = model.predict(img)\n",
    "index_predicted_img = np.argmax(predicted_img)\n",
    "print(CATEGORIES[index_predicted_img])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
